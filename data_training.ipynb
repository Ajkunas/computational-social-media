{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff158974",
   "metadata": {},
   "source": [
    "## Features conversion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda8ca4",
   "metadata": {},
   "source": [
    "We compare 3 techniques :\n",
    "\n",
    "* Bag of Words features : learns a vocabulary form of all the documents, then models each document by counting the number of times each word appears\n",
    "\n",
    "* TF-IDF features : words are given weight TF-IDF measures relevance, not frequency. Method for emphasizing words that occur frequently in a given document, wile deemphasizing words that occur frequently in many documents \n",
    "\n",
    "* Word2Vec features: combination of two techniques, CBOW and Skip gram model. Both are neural networks which map words to the target variables which is also a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9e1ba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    56011\n",
       " 0    55487\n",
       "-1    53898\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/preprocessed_sentiment.csv', usecols=['tweets','labels'])\n",
    "df.labels.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78c75d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14499</th>\n",
       "      <td>imagin abl creat type product want comfort hom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39989</th>\n",
       "      <td>openai api die right now timeout everywher dav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58769</th>\n",
       "      <td>student room phone smart watch radio receiv pr...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16408</th>\n",
       "      <td>agre ryan here imho chatgpt gpt solv key biote...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148013</th>\n",
       "      <td>chatgpt good imit everyth itself small step cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweets  labels\n",
       "14499   imagin abl creat type product want comfort hom...       1\n",
       "39989   openai api die right now timeout everywher dav...       1\n",
       "58769   student room phone smart watch radio receiv pr...      -1\n",
       "16408   agre ryan here imho chatgpt gpt solv key biote...       1\n",
       "148013  chatgpt good imit everyth itself small step cl...       0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1df23583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any NaN value\n",
    "\n",
    "df.tweets.isnull().values.any()\n",
    "df.tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0918f23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    56011\n",
       " 0    55487\n",
       "-1    53892\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the NaN values if any\n",
    "\n",
    "df = df.dropna()\n",
    "df.labels.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67c0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words : \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim\n",
    "\n",
    "# need to tune the parameters \n",
    "BOWvectorize = CountVectorizer(max_df = 0.90, min_df = 2, max_features = 1000, stop_words='english')\n",
    "BOW = BOWvectorize.fit_transform(df.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8601c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165390, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09876dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF features: \n",
    "TfidfVect = TfidfVectorizer(max_df = 0.90, min_df = 2, max_features = 1000, stop_words='english')\n",
    "Tfidf = TfidfVect.fit_transform(df.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8dc97e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165390, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c775c25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32280296, 38528160)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec features: \n",
    "tokenize_tweet = df.tweets.apply(lambda x: x.split())\n",
    "\n",
    "model_W2V = gensim.models.Word2Vec(tokenize_tweet, \n",
    "                                   vector_size = 200, # No. of features\n",
    "                                   window =  5, # default window\n",
    "                                   min_count = 2, \n",
    "                                   sg = 1, # 1 for skip-gram model\n",
    "                                   hs = 0,\n",
    "                                   negative = 10, # for negative sampling\n",
    "                                   workers = 2,  # No. of cores\n",
    "                                   seed = 34 )\n",
    "\n",
    "model_W2V.train(tokenize_tweet, total_examples= len(df.tweets), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c03ae82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  34919\n",
      "sample words  ['chatgpt', 'openai', 'write', 'ask', 'like', 'use', 'googl', 'new', 'gener', 'code', 'it', 'chatbot', 'answer', 'good', 'time', 'question', 'think', 'know', 'work', 'tri', 'creat', 'thing', 'help', 'gpt', 'human', 'peopl', 'search', 'way', 'tool', 'need', 'learn', 'amp', 'model', 'better', 'futur', 'intellig', 'world', 'languag', 'day', 'bot', 'go', 'play', 'prompt', 'chat', 'technolog', 'year', 'look', 'want', 'talk', 'amaz']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(model_W2V.wv.index_to_key)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8dd786a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 165390/165390 [04:11<00:00, 657.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165390\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "vector = []\n",
    "for sent in tqdm(tokenize_tweet):\n",
    "    sent_vec = np.zeros(200)\n",
    "    count = 0\n",
    "    for word in sent: \n",
    "        if word in w2v_words:\n",
    "            vec = model_W2V.wv[word]\n",
    "            sent_vec += vec \n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        sent_vec /= count #normalize\n",
    "    vector.append(sent_vec)\n",
    "    \n",
    "print(len(vector))\n",
    "print(len(vector[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c5c13",
   "metadata": {},
   "source": [
    "## Model building and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30379151",
   "metadata": {},
   "source": [
    "For the three types of embedding features method, we will compare the following models: \n",
    "* Logistic Regression \n",
    "* SVM \n",
    "* Random Forest \n",
    "* KNN \n",
    "* CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d2fe4",
   "metadata": {},
   "source": [
    "### BOW classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f697c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "x_train_bow, x_test_bow, y_train_bow, y_test_bow = train_test_split(BOW, df.labels, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74bbce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajkuna Seipi\\AppData\\Local\\Temp\\ipykernel_8804\\2930640125.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pred = pred.astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17800350686256725"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "LR = LogisticRegression(solver='lbfgs', max_iter=500, multi_class='multinomial')\n",
    "LR.fit(x_train_bow, y_train_bow)\n",
    "\n",
    "prediction = LR.predict_proba(x_test_bow)\n",
    "\n",
    "# if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "pred = prediction[:,1] >= 0.3\n",
    "pred = pred.astype(np.int)\n",
    "\n",
    "f1_score(y_test_bow, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ddc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM for BOW \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#parameters in SVC\n",
    "# c_list=list(range(1,51))\n",
    "param_grid_svc = {'C': [1, 10, 100, 1000],\n",
    "                  'kernel': ['linear','poly','rbf','sigmoid'],\n",
    "                  'degree': [1,2,3,4]}\n",
    "print(param_grid_svc)\n",
    "\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13917125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters for SVC\n",
    "SVC_RandomGrid = RandomizedSearchCV(estimator = model_svc, param_distributions = param_grid_svc, cv = 10, verbose=2, n_jobs = 4)\n",
    "SVC_RandomGrid.fit(x_train_bow, y_train_bow)\n",
    "SVC_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f60ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc = SVC(kernel='linear',degree=1, C=1)\n",
    "model_svc = model_svc.fit(x_train_bow, y_train_bow)\n",
    "prediction_svc = model_svc.predict(x_test_bow)\n",
    "\n",
    "print(classification_report(y_test_bow, prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest for BOW: \n",
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "\n",
    "model_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameter for RF\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#parameters in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] #tree number\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "max_depth = [10,20,30,40]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "# Create the param grid\n",
    "param_grid_forest = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "RF_RandomGrid = RandomizedSearchCV(estimator = model_forest, param_distributions = param_grid_forest, cv = 10, verbose=2, n_jobs = 4)\n",
    "RF_RandomGrid.fit(x_train_bow, y_train_bow)\n",
    "RF_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model establishment and results \n",
    "# Random Forest\n",
    "model_forest = RandomForestClassifier(n_estimators = 140, min_samples_split=10, min_samples_leaf=2, max_features='auto', max_depth=40)\n",
    "model_forest.fit(x_train_bow, y_train_bow)\n",
    "prediction = model_forest.predict(x_test_bow)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test_bow, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
