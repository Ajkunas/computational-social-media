{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88718619",
   "metadata": {},
   "source": [
    "## Features conversion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e02933",
   "metadata": {},
   "source": [
    "We compare 3 techniques :\n",
    "\n",
    "* Bag of Words features : learns a vocabulary form of all the documents, then models each document by counting the number of times each word appears\n",
    "\n",
    "* TF-IDF features : words are given weight TF-IDF measures relevance, not frequency. Method for emphasizing words that occur frequently in a given document, wile deemphasizing words that occur frequently in many documents \n",
    "\n",
    "* Word2Vec features: combination of two techniques, CBOW and Skip gram model. Both are neural networks which map words to the target variables which is also a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64326b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    56011\n",
       " 0    55487\n",
       "-1    53898\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/preprocessed_sentiment.csv', usecols=['tweets','labels'])\n",
    "df.labels.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d5144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148362</th>\n",
       "      <td>bit python code today deep rabbit hole pyplot ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81991</th>\n",
       "      <td>learn absolut fun prompt today nnignor previou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71397</th>\n",
       "      <td>chatgpt good free capitalist perspect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76924</th>\n",
       "      <td>hey check cool site found topic viamytwitternam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58997</th>\n",
       "      <td>elon musk found critic compani buzzi new chatb...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweets  labels\n",
       "148362  bit python code today deep rabbit hole pyplot ...      -1\n",
       "81991   learn absolut fun prompt today nnignor previou...       1\n",
       "71397               chatgpt good free capitalist perspect       1\n",
       "76924     hey check cool site found topic viamytwitternam       0\n",
       "58997   elon musk found critic compani buzzi new chatb...      -1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07ce510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any NaN value\n",
    "\n",
    "df.tweets.isnull().values.any()\n",
    "df.tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e256825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    56011\n",
       " 0    55487\n",
       "-1    53892\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the NaN values if any\n",
    "\n",
    "df = df.dropna()\n",
    "df.labels.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44c5f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words : \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim\n",
    "\n",
    "# need to tune the parameters \n",
    "BOWvectorize = CountVectorizer(max_df = 0.90, min_df = 2, max_features = 1000, stop_words='english')\n",
    "BOW = BOWvectorize.fit_transform(df.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead8fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165390, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d3acfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF features: \n",
    "TfidfVect = TfidfVectorizer(max_df = 0.90, min_df = 2, max_features = 1000, stop_words='english')\n",
    "Tfidf = TfidfVect.fit_transform(df.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb5b86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165390, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df68bc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32281683, 38528160)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec features: \n",
    "tokenize_tweet = df.tweets.apply(lambda x: x.split())\n",
    "\n",
    "model_W2V = gensim.models.Word2Vec(tokenize_tweet, \n",
    "                                   vector_size = 200, # No. of features\n",
    "                                   window =  5, # default window\n",
    "                                   min_count = 2, \n",
    "                                   sg = 1, # 1 for skip-gram model\n",
    "                                   hs = 0,\n",
    "                                   negative = 10, # for negative sampling\n",
    "                                   workers = 2,  # No. of cores\n",
    "                                   seed = 34 )\n",
    "\n",
    "model_W2V.train(tokenize_tweet, total_examples = len(df.tweets), epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d41d2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each word can get its own vector. The representation of a tweets can the vector sum of each word divided by the total number(average) \n",
    "#or just the sum of each word vector\n",
    "\n",
    "def word2vec_tweet(tokens, size):\n",
    "    vector = np.zeros(size).reshape((1,size))\n",
    "    vector_cnt = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vector += model_W2V.wv[word].reshape((1, size))\n",
    "            vector_cnt += 1\n",
    "            \n",
    "        except KeyError: \n",
    "            print(word, 'not found')\n",
    "            \n",
    "    if vector_cnt != 0:\n",
    "        vector = vector/vector_cnt #average for tweets\n",
    "    \n",
    "    return vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3e49858",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "84349",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 84349",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8804\\3494728790.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtweet_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtweet_vec_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 84349"
     ]
    }
   ],
   "source": [
    "tweet_arr = np.zeros((len(tokenize_tweet), 200))\n",
    "\n",
    "for i in range (len(tokenize_tweet)):\n",
    "    tweet_arr[i,:] = word2vec_tweet(tokenize_tweet[i], 200)\n",
    "    \n",
    "tweet_vec_df = pd.DataFrame(tweet_arr)\n",
    "tweet_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bc7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
