{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5c7c30",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69aa295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# remove english stopwords with the help of the gensim library \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be3035",
   "metadata": {},
   "source": [
    "## All the methods that will be used to preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ac46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab93ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and removing URLs\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "\n",
    "def remove_hyperlink(word):\n",
    "    return re.sub(r\"http\\S+\", \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "598b5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and removing mentions \n",
    "\n",
    "def remove_mentions(word):\n",
    "    return re.sub(r\"@\\S+\", \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff90e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing emojis \n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3572bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and removing repeating characters\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e5dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723bec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lematize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cf483",
   "metadata": {},
   "source": [
    "## Defining the function for preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39a5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocessing(df):\n",
    "    # lower cases \n",
    "    df = df.str.lower()\n",
    "    print(\"Text of tweets transformed to lower cases.\")\n",
    "    \n",
    "    # remove english stopwords with the help of the gensim library \n",
    "    df = df.apply(lambda text: remove_stopwords(text))\n",
    "    print(\"Removed stopwords from the text of tweets.\")\n",
    "    \n",
    "    #cleaning and removing punctuation\n",
    "    df = df.apply(lambda x: cleaning_punctuations(x))\n",
    "    print(\"Cleaned and removed the punctuations.\")\n",
    "    \n",
    "    # cleaning and removing URLs\n",
    "    df = df.apply(lambda x: cleaning_URLs(x))\n",
    "    df = df.apply(lambda x: remove_hyperlink(x))\n",
    "    print(\"Cleaned and removed the URLs and hyperlinks.\")\n",
    "    \n",
    "    # cleaning and removing mentions \n",
    "    df = df.apply(lambda x: remove_mentions(x))\n",
    "    print(\"Cleaned and removed the mentions.\")\n",
    "    \n",
    "    # removing emojis \n",
    "    df = df.apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "    print(\"Removed the emojis.\")\n",
    "    \n",
    "    # remove numbers \n",
    "    df = df.apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "    print(\"Removed the numbers.\")\n",
    "    \n",
    "    # cleaning and removing repeating characters\n",
    "    df = df.apply(lambda x: cleaning_repeating_char(x))\n",
    "    print(\"Cleaned and removed the repeating characters.\")\n",
    "    \n",
    "    # remove short words that are of no use \n",
    "    df = df.apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    print(\"Removed the short words (length less than 3).\")\n",
    "    \n",
    "    # Text normalization : tokenize the tweets \n",
    "    # split tweets of text into tokens (individual words of terms)\n",
    "    tokenize_tweets = df.apply(lambda x: x.split())\n",
    "    print(\"Tokenized the text.\")\n",
    "    \n",
    "    # Normalization: use of PorterStemmer()\n",
    "    tokenize_tweets = tokenize_tweets.apply(lambda text: lematize_words(text))\n",
    "    print(\"Normalized the text.\")\n",
    "    \n",
    "    # write new csv file containing the preprocessed dataset\n",
    "    #df.to_csv(path)\n",
    "    \n",
    "    return tokenize_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78a25b",
   "metadata": {},
   "source": [
    "### Preprocessing of the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0de5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/chatgpt.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847849d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400775"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14ae8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name               13\n",
       "text                    42\n",
       "user_location       112641\n",
       "user_description     25950\n",
       "user_created            60\n",
       "user_followers          60\n",
       "user_friends            60\n",
       "user_favourites         60\n",
       "user_verified           67\n",
       "date                    68\n",
       "hashtags             90810\n",
       "source                 106\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any NaN value\n",
    "\n",
    "df.isnull().values.any()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be59cc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_name', 'text', 'user_location', 'user_description',\n",
       "       'user_created', 'user_followers', 'user_friends', 'user_favourites',\n",
       "       'user_verified', 'date', 'hashtags', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3517aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user_name', 'text', 'user_description', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d807d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name              13\n",
       "text                   42\n",
       "user_description    25950\n",
       "date                   68\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a58f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "281aea85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name           0\n",
       "text                0\n",
       "user_description    0\n",
       "date                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4754f36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374777"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aa5cbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_name', 'text', 'user_description', 'date'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8315804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = df['user_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9e08a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = df['user_description']\n",
    "text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d33adb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any NaN value\n",
    "description.isnull().values.any()\n",
    "description.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b05f238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.isnull().values.any()\n",
    "user.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05cea949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any NaN value\n",
    "text.isnull().values.any()\n",
    "text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fd52875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of tweets transformed to lower cases.\n",
      "Removed stopwords from the text of tweets.\n",
      "Cleaned and removed the punctuations.\n",
      "Cleaned and removed the URLs and hyperlinks.\n",
      "Cleaned and removed the mentions.\n",
      "Removed the emojis.\n",
      "Removed the numbers.\n",
      "Cleaned and removed the repeating characters.\n",
      "Removed the short words (length less than 3).\n",
      "Tokenized the text.\n",
      "Normalized the text.\n",
      "Text of tweets transformed to lower cases.\n",
      "Removed stopwords from the text of tweets.\n",
      "Cleaned and removed the punctuations.\n",
      "Cleaned and removed the URLs and hyperlinks.\n",
      "Cleaned and removed the mentions.\n",
      "Removed the emojis.\n",
      "Removed the numbers.\n",
      "Cleaned and removed the repeating characters.\n",
      "Removed the short words (length less than 3).\n",
      "Tokenized the text.\n",
      "Normalized the text.\n",
      "Text of tweets transformed to lower cases.\n",
      "Removed stopwords from the text of tweets.\n",
      "Cleaned and removed the punctuations.\n",
      "Cleaned and removed the URLs and hyperlinks.\n",
      "Cleaned and removed the mentions.\n",
      "Removed the emojis.\n",
      "Removed the numbers.\n",
      "Cleaned and removed the repeating characters.\n",
      "Removed the short words (length less than 3).\n",
      "Tokenized the text.\n",
      "Normalized the text.\n"
     ]
    }
   ],
   "source": [
    "clean_description = preprocessing(description)\n",
    "clean_text = preprocessing(text)\n",
    "clean_user = preprocessing(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aff7cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da520299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         building draftmateai developer author chatgpt ...\n",
       "1         nomadic marketing vigilante graphic web design...\n",
       "3         leading translation management platform locali...\n",
       "4         engineering leader coach mentor building remot...\n",
       "5         stop learning research develop uild future eyo...\n",
       "                                ...                        \n",
       "400769    brain meant processing million tweet post vide...\n",
       "400770    blockchain enthusiast philanthropist slave jav...\n",
       "400772    mathematician developer amazon previously geob...\n",
       "400773      passionate nature software developer profession\n",
       "400774    postdoc gipplab unigoettingen phd unikonstanz ...\n",
       "Name: user_description, Length: 374777, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c729696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clean_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "770f3e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['building draftmateai developer author chatgpt starter kit laravel',\n",
       "       'nomadic marketing vigilante graphic web designer coo kapturall',\n",
       "       'leading translation management platform localize content device platform',\n",
       "       ..., 'mathematician developer amazon previously geoblink',\n",
       "       'passionate nature software developer profession',\n",
       "       'postdoc gipplab unigoettingen phd unikonstanz previously ucberkeley jouhouken uniwuppertal interest nlp datascience'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_description.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "631091a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-04-26 13:02:16+00:00', '2023-04-26 13:02:06+00:00',\n",
       "       '2023-04-26 13:02:05+00:00', ..., '2022-12-05 17:09:04+00:00',\n",
       "       '2022-12-05 17:08:44+00:00', '2022-12-05 17:08:20+00:00'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e86d7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                bilal haidar\n",
       "1              josh arrington\n",
       "3                   smartling\n",
       "4                     richard\n",
       "5                    lockroca\n",
       "                 ...         \n",
       "400769                   nenu\n",
       "400770               iamtmoyo\n",
       "400772    gabriel furstenheim\n",
       "400773                 devang\n",
       "400774        norman meuschke\n",
       "Name: user_name, Length: 374777, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "536a8209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I'm happy to share with you that @draftmateai ...\n",
       "1         🤖 - Integrate ChatGPT into your Marketo Smart ...\n",
       "3         We’re pleased to announce new patent-pending t...\n",
       "4         Starting to tire of being scolded by ChatGPT f...\n",
       "5         @SpartaJustice It's notable that wikipedia ent...\n",
       "                                ...                        \n",
       "400769    ChatGPT is the biggest, smartest brain 🧠 in th...\n",
       "400770    Levels🙏🙏🙏,so happy for the chatGPT team for co...\n",
       "400772    Russel vs ChatGPT. It's also funny that it tak...\n",
       "400773    Was just wondering is there any difference bet...\n",
       "400774    #ChatGPT and similar #LLM pose a challenge to ...\n",
       "Name: text, Length: 374777, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c214d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I'm happy to share with you that @draftmateai now supports a 5-day free trial period to try out the application before you subscribe!\\n\\nCheck it here: https://t.co/xzXrnE5ATY. \\n\\n#ChatGPT #AI #Laravel\",\n",
       "       '🤖 - Integrate ChatGPT into your Marketo Smart Campaigns! Discover the power of AI-driven marketing in our upcoming webinar on May 4th at 4 PM CEST. Save your spot today -\\xa0https://t.co/cjCFZ8rH1U\\n\\n#AI\\xa0#Marketo\\xa0#ChatGPT\\xa0#OpenAI https://t.co/YA91h1K8cq',\n",
       "       \"We’re pleased to announce new patent-pending technology that will bring human-quality machine translation closer to reality.\\n\\nThis new AI-powered process enables the use of #LLMs (GPT-4, #ChatGPT) to follow companys' style guides and maintain brand voice.\\n\\nhttps://t.co/gnKqAch4Yy\",\n",
       "       ...,\n",
       "       \"Russel vs ChatGPT. It's also funny that it takes a long time to answer\\n#ChatGPT https://t.co/2YkR0fRVCT\",\n",
       "       'Was just wondering is there any difference between Jasper and ChatGPT / GPT3? #ChatGPT #GPT3',\n",
       "       '#ChatGPT and similar #LLM pose a challenge to academic integrity. Jan Wahle explains why our current research (presented at #EMNLP later this week) makes us optimistic that this challenge can be met. https://t.co/MEe0s9d5EW'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7762de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = {'user_name': clean_user.values, 'user_description': clean_description.values, 'text': text.values, 'clean_text': clean_text.values, 'date': date.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "505c0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bb41fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('datasets/preprocessed_chatgpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b81b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
